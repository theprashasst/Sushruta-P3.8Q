{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "12nR-yRnlVHuUmOt6R0qS_hR5nELKqfpK",
      "authorship_tag": "ABX9TyOCBEAoq/HJZ1QSysHzzIrb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theprashasst/Sushruta-P3.8Q/blob/main/Quantization%20and%20LoRA%20Finetuning%20(QLoRA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vHs6pShkwbCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================================\n",
        "# FINAL SCRIPT: Fine-Tuning Sushruta-P3.8Q with QLoRA and W&B Integration\n",
        "#\n",
        "# Creator: Prashasst Dongre, Prashasst's AI Labs\n",
        "# Timestamp: June 22, 2025\n",
        "#\n",
        "# This script does everything: installs dependencies, logs in, configures the\n",
        "# model and persona, loads and prepares the dataset, and starts the training.\n",
        "# ====================================================================================\n"
      ],
      "metadata": {
        "id": "aynpsx3xUEMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 0: Install All Required Libraries ---\n",
        "print(\"--- [0/7] Installing required libraries... ---\")\n",
        "# First, install a stable and compatible PyTorch ecosystem for Colab's CUDA version\n",
        "!pip install -q -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "# Now, install all other necessary ML libraries\n",
        "!pip install -q -U transformers accelerate bitsandbytes peft trl datasets wandb\n",
        "print(\"✅ All libraries installed successfully.\")"
      ],
      "metadata": {
        "id": "dMqN6h_qUCD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Import Libraries & Log In ---\n",
        "print(\"\\n--- [1/7] Importing libraries & Logging In... ---\")\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "import wandb\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import login\n",
        "\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "WNB=userdata.get('WNB')\n",
        "print(\"✅ Libraries imported successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Log in to Hugging Face Hub to download the model\n",
        "print(\"--> Please log in to your Hugging Face account:\")\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# Log in to Weights & Biases to track the experiment\n",
        "print(\"\\n--> Please log in to your Weights & Biases account:\")\n",
        "wandb.login(key=WNB)\n",
        "print(\"✅ Logins successful.\")"
      ],
      "metadata": {
        "id": "0BRMjs3VUNnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Define All Configurations ---\n",
        "print(\"\\n--- [2/7] Setting up all configurations... ---\")\n",
        "\n",
        "# W&B Project Name\n",
        "os.environ[\"WANDB_PROJECT\"] = \"Sushruta-P3.8Q-Finetune\"\n",
        "\n",
        "# Persona and Identity Configuration\n",
        "GUIDELINES = \"\"\"\n",
        "# IDENTITY AND ORIGIN\n",
        "You are Sushruta-P3.8Q, a specialized medical large language Model from Prashasst's AI Labs. You were created and fine-tuned by Prashasst Dongre to serve as a reliable and accessible educational tool for the public.\n",
        "# CORE MISSION\n",
        "Your primary purpose is to help any user understand complex medical topics by providing clear, logical, step-by-step analyses.\n",
        "# STRICT OPERATIONAL GUIDELINES\n",
        "1.  Greet users politely and neutrally.\n",
        "2.  If asked about your origin, state you were created by Prashasst Dongre at Prashasst's AI Labs.\n",
        "3.  Always follow the Chain-of-Thought format: <think>...</think> followed by <solution>...</solution>.\n",
        "4.  Politely refuse non-medical questions, stating that your function is specialized.\n",
        "5.  Frame all responses as educational explanations, not direct medical advice.\n",
        "# PERSONA AND TONE\n",
        "- **Tone:** Your tone must be professional, empathetic, precise, and trustworthy.\n",
        "- **Persona:** You are a public-facing specialist: knowledgeable, safe, and helpful to all users.\n",
        "\"\"\"\n",
        "\n",
        "# Model and Quantization Configuration\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "load_in_4bit=True,                   # This is the master switch to enable 4-bit quantization\n",
        "bnb_4bit_quant_type=\"nf4\",           # nf4 is a special 4-bit data type designed for neural networks\n",
        "bnb_4bit_compute_dtype=torch.bfloat16, # The compute type is bfloat16 for speed and stability\n",
        "bnb_4bit_use_double_quant=True,      # A memory-saving technique within quantization\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "BXNSS023VeUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Load Base Model & Tokenizer ---\n",
        "print(\"\\n--- [3/7] Loading base model and tokenizer... ---\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "print(\"✅ Base model and tokenizer loaded.\")\n",
        "# Get the memory footprint in bytes and convert to gigabytes\n",
        "memory_bytes = model.get_memory_footprint()\n",
        "memory_gb = memory_bytes / (1024**3)\n",
        "\n",
        "print(f\"\\nMemory footprint of the 4-bit model: {memory_gb:.2f} GB\")\n",
        "print(\"✅ Configurations are set.\")"
      ],
      "metadata": {
        "id": "3CSF4KxAVhzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Load and Prepare the Dataset ---\n",
        "print(\"\\n--- [4/7] Loading and reformatting the dataset... ---\")\n",
        "dataset = load_dataset(\"FreedomIntelligence/Medical-R1-Distill-Data\", split=\"train\")\n",
        "\n",
        "def reformat_dataset(example):\n",
        "    question = example['question']\n",
        "    thinking_process = example['reasoning (reasoning_content)']\n",
        "    final_answer = example['response (content)']\n",
        "    answer = f\"<think>{thinking_process}</think><solution>{final_answer}</solution>\"\n",
        "    return {\"question\": question, \"answer\": answer}\n",
        "\n",
        "formatted_dataset = dataset.map(reformat_dataset)\n",
        "print(\"✅ Dataset loaded and formatted.\")\n"
      ],
      "metadata": {
        "id": "2xV8nTg9VlfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5.1: Filter Long Sequences (NEW STEP) ---\n",
        "print(\"\\n--- [5/8] Filtering out overly long examples... ---\")\n",
        "# This function tokenizes the full prompt and checks its length\n",
        "def filter_long_examples(example):\n",
        "    prompt = f\"<|system|>\\n{GUIDELINES}<|end|>\\n<|user|>\\n{example['question']}<|end|>\\n<|assistant|>\\n{example['answer']}<|end|>\"\n",
        "    # The tokenizer returns a dictionary with 'input_ids', we check the length of that list\n",
        "    return len(tokenizer(prompt)['input_ids']) <= 4096\n",
        "\n",
        "final_dataset = formatted_dataset.filter(filter_long_examples)\n",
        "print(f\"Original dataset size: {len(formatted_dataset)}\")\n",
        "print(f\"Filtered dataset size: {len(final_dataset)}\")\n",
        "print(\"✅ Long examples removed.\")"
      ],
      "metadata": {
        "id": "49fZHdZaZ5EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 5: Configure LoRA Adapters ---\n",
        "print(\"\\n--- [5/7] Configuring LoRA adapters... ---\")\n",
        "model.config.use_cache = False\n",
        "peft_model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "peft_model = get_peft_model(peft_model, lora_config)\n",
        "print(\"✅ LoRA adapters configured and applied.\")"
      ],
      "metadata": {
        "id": "bA8SCGOjWIJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Step 6: Configure the SFT Trainer ---\n",
        "print(\"\\n--- [6/7] Configuring the Trainer... ---\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/Sushruta-P3.8Q-Finetune\", # <-- Saves to your Google Drive!\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    max_steps=200,      # Train for 200 steps for a solid result. Increase later for higher quality.\n",
        "    logging_steps=10,    # Log to W&B every 10 steps\n",
        "    save_steps=100,      # Save a checkpoint every 100 steps\n",
        "    fp16=True,           # Use mixed precision for speed\n",
        "    report_to=\"wandb\",   # Enable W&B logging\n",
        "    run_name=f\"Sushruta-P3.8Q-{int(time.time())}\" # Descriptive run name\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=formatted_dataset,\n",
        "    peft_config=lora_config,\n",
        "    # max_seq_length=2048, # Increased for the detailed medical dataset\n",
        "    # tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    formatting_func=lambda example: f\"<|system|>\\n{GUIDELINES}<|end|>\\n<|user|>\\n{example['question']}<|end|>\\n<|assistant|>\\n{example['answer']}<|end|>\"\n",
        ")\n",
        "print(\"✅ Trainer is ready.\")\n"
      ],
      "metadata": {
        "id": "qbB5IZl1WMX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Step 7: Start Fine-Tuning ---\n",
        "print(\"\\n--- [7/7] Starting the fine-tuning process! ---\")\n",
        "print(\"This will take a while. A link to your W&B dashboard will appear below.\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\\n--- 🎉 TRAINING COMPLETE! 🎉 ---\")\n",
        "wandb.finish() # Properly finish the W&B run\n",
        "trainer.save_model(\"./Sushruta-P3.8Q-final-adapters\")\n",
        "print(\"Final model adapters have been saved to './Sushruta-P3.8Q-final-adapters'\")"
      ],
      "metadata": {
        "id": "2A5EhbYQRt-7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}